{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088cc654",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#工具函数\" data-toc-modified-id=\"工具函数-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Utility Funcations(工具函数)</a></span></li><li><span><a href=\"#宏变量配置\" data-toc-modified-id=\"宏变量配置-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Macro Variables(宏变量配置)</a></span><ul class=\"toc-item\"><li><span><a href=\"#固定配置参数\" data-toc-modified-id=\"固定配置参数-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Fixed Parameters(固定配置参数)</a></span></li><li><span><a href=\"#可调节超参数\" data-toc-modified-id=\"可调节超参数-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Hyperparameters(可调节超参数)</a></span></li></ul></li><li><span><a href=\"#准备工作\" data-toc-modified-id=\"准备工作-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preparation Works(准备工作)</a></span><ul class=\"toc-item\"><li><span><a href=\"#获取所有列名\" data-toc-modified-id=\"获取所有列名-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Obtain Columns(获取所有列名)</a></span></li><li><span><a href=\"#数据集划分\" data-toc-modified-id=\"数据集划分-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Data Set Split(数据集划分)</a></span></li></ul></li><li><span><a href=\"#特征筛选\" data-toc-modified-id=\"特征筛选-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature Selection(特征筛选)</a></span></li><li><span><a href=\"#效果验证\" data-toc-modified-id=\"效果验证-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Check Performance(效果验证)</a></span><ul class=\"toc-item\"><li><span><a href=\"#训练模型\" data-toc-modified-id=\"训练模型-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Model Training(训练模型)</a></span></li><li><span><a href=\"#TOPK效果\" data-toc-modified-id=\"TOPK效果-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>TOPK Result(TOPK效果)</a></span></li><li><span><a href=\"#规则解释\" data-toc-modified-id=\"规则解释-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Rule Explaination(规则解释)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "668a4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a934f19",
   "metadata": {},
   "source": [
    "# Utility Funcations (工具函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762a4f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================【1. Utility Functions】=============================\n",
    "def seed_everything(seed=2022):\n",
    "    \"\"\"_summary_ \n",
    "    set random seed, used for reproduction and comparison\n",
    "    (设定随机种子,便于复现和进行对比实验)\n",
    "    Args:\n",
    "        seed (int, optional):random seed (随机种子). Defaults to 2022.\n",
    "    \"\"\"\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    \n",
    "def read_chunk_data(data_path, use_cols=None, chunksize=None, nrows=None, sep=','):\n",
    "    \"\"\"_summary_\n",
    "    read csv file in chunks\n",
    "    分列/分块读取csv文件\n",
    "    Args:\n",
    "        data_path (str): csv files(source files) path. (数据文件目录地址)\n",
    "        use_cols (List, optional): columns to read, Defaults to None. (读取的列名,默认读取全部列). \n",
    "        chunksize (int, optional): rows to read in a chunk. (分块读取csv文件的行数,默认不分快，一次读取全部行)\n",
    "        nrows (int, optional): Number of rows of file to read. (指定读取行数,默认读取不分块)\n",
    "        sep (str, optional): csv seperator, Defaults to ','. (文件分隔符). \n",
    "\n",
    "    Returns:\n",
    "        DataFrame: 读取后的dataframe文件\n",
    "    \"\"\"\n",
    "    print(\"{0:=^70}\".format(f'LOADING DATA FROM : {data_path}'))\n",
    "    \n",
    "    # when chunksize is not None, read csv in chunks\n",
    "    if chunksize:\n",
    "        data = []\n",
    "        chunk = pd.read_csv(data_path, usecols=use_cols, nrows=nrows, sep=sep, chunksize=chunksize)\n",
    "        \n",
    "        # iterate through chunks\n",
    "        for tmp_data in chunk:\n",
    "            data.append(tmp_data)\n",
    "        \n",
    "        data = pd.concat(data, axis=0)  # merge csv\n",
    "        del tmp_data; gc.collect()  # release memory\n",
    "    \n",
    "    # read the whole csv when chunksize is None\n",
    "    else:\n",
    "        data = pd.read_csv(data_path, usecols=use_cols, nrows=nrows, sep=sep, chunksize=chunksize)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================【2. Feature Engineering Functions】=============================\n",
    "def label_encode(df, cols, map_dict=None):\n",
    "    \"\"\"_summary_\n",
    "    categorical encoding (类别特征编码)\n",
    "    Args:\n",
    "        df (dataframe):data files (数据文件)\n",
    "        cols (list): column names for categorical features (类别特征名称)\n",
    "        map_dict (dict, optional): a map for categorical features. None when training, \n",
    "        use the map from after training when doing the inference\n",
    "        (类别特征编码字典,训练时为None,推理时应传入训练时返回的dict). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dataframe: encoding result for categorical features (类别特征编码完成后的数据)\n",
    "        dict: a map for categorical features and its encoding (类别特征编码对应关系)\n",
    "    \"\"\"\n",
    "    print(\"{0:=^70}\".format(f'LABEL ENCODING FOR {len(cols)} FEARURES'))\n",
    "    \n",
    "    # CallBack when inference (在推理时map_dict不为空，因此只需要遍历ap_dict就行)\n",
    "    if map_dict is not None:\n",
    "        for col in tqdm_notebook(cols):\n",
    "            df[col] = df[col].map(map_dict[col])\n",
    "    \n",
    "    # None when Training 训练\n",
    "    else:\n",
    "        map_dict = {}\n",
    "        \n",
    "        # iterate through categoricals (遍历所有类别特征)\n",
    "        for col in tqdm_notebook(cols):\n",
    "            # 1. df.sample(frac=1.0): shuffle, in case of Data leakage (对数据进行打乱，防止出现标签泄露)\n",
    "            # 2. df.unique(): unique categorical values (类别特征取值)\n",
    "            # 3. zip(): mapping a categorical to an int (将类别特征取值与一个int对应)\n",
    "            map_dict[col] = dict(zip(df[col].sample(frac=1.0).unique(), range(len(df[col].sample(frac=1.0).unique()))))\n",
    "            # 编码\n",
    "            df[col] = df[col].map(map_dict[col])\n",
    "            \n",
    "    return df, map_dict\n",
    "\n",
    "\n",
    "def filter_cols(df, use_cols, cols, ratio):\n",
    "    \"\"\"_summary_\n",
    "    filter abnormal features\n",
    "    (过滤异常特征)\n",
    "    Args:\n",
    "        df (dataframe): data files (数据文件)\n",
    "        use_cols (list): all columns in df (传入文件的所有列)\n",
    "        cols (list): columns need to filter (需要进行过滤的文件(一般来说与use_cols相等))\n",
    "        ratio (float): missing ratio to filter (缺失值过滤比例(缺失占比大于一定程度就过滤该特征))\n",
    "\n",
    "    Returns:\n",
    "        dataframe: df adter removing the abnormal columns 除去异常列的数据\n",
    "        list: column names after filtering 过滤完成的列名\n",
    "    \"\"\"\n",
    "    print(\"{0:=^70}\".format(f'FILTER {len(cols)} FEARURES'))\n",
    "    \n",
    "    useless_cols = []  # abnormal columns 异常列名\n",
    "    \n",
    "    # iterate over columns 遍历待过滤的所有列\n",
    "    for col in tqdm_notebook(cols):\n",
    "        # firter the column when missing ratio over the set ratio\n",
    "        #(缺失值占比大于设定值则去除该列)\n",
    "        if df[col].isnull().sum() / df.shape[0] > ratio:\n",
    "            useless_cols.append(col)\n",
    "        \n",
    "        # firter the column when there is only one value(该列取值唯一时去除该列)\n",
    "        elif df[col].nunique() == 1:\n",
    "            useless_cols.append(col)\n",
    "            \n",
    "    print(f'Drop {len(useless_cols)} features')\n",
    "    use_cols = [i for i in use_cols if i not in useless_cols]  # remained columns(过滤后的列名)\n",
    "    df.drop(useless_cols, axis=1, inplace=True)  # drop abnormal columns(删除所有异常列)\n",
    "    \n",
    "    return df, use_cols\n",
    "\n",
    "def filter_data(data, date_col=None, window=None, filter_=None):\n",
    "    \"\"\"_summary_\n",
    "    filter accounts that are low risks\n",
    "    (前置过滤 提前过滤低风险账户+天)\n",
    "    Args:\n",
    "        data (dataframe): data files (数据文件)\n",
    "        date_col (str, optional): datetime columns, Defaults to None. (日期列列名, 使用时间窗口时需要指定). \n",
    "        window (int, optional): datetime window size, Defaults to None. (时间窗口大小). \n",
    "        filter_ (str, optional): filter conditions, Defaults to None. (前置过滤条件). \n",
    "\n",
    "    Returns:\n",
    "        dataframe: filtered files (过滤完成的数据文件)\n",
    "    \"\"\"\n",
    "    print(\"{0:=^70}\".format('FILTER date'))\n",
    "    print(f'start shape {data.shape}')\n",
    "    \n",
    "    # when window is not None, reserve trade data of window size days prior to the last trade\n",
    "    # (当window不为空时，保留每个用户最后一次交易的近window天数据)\n",
    "    if window is not None:\n",
    "        assert date_col is not None, 'When the window is not empty, the date_col must not be empty.'\n",
    "        \n",
    "        data[date_col] = pd.to_datetime(data[date_col], format='%Y/%m/%d')  # str2time\n",
    "        # order by trade date in descending order (每个用户按日期倒叙排列的序值)\n",
    "        data['rnk'] = data.groupby(UID)[date_col].rank(ascending = False)\n",
    "        # reserve window size days of trade data (保留每个用户最后一次交易的近window天数据)\n",
    "        data = data.query(f'rnk <= {window}')\n",
    "        del data['rnk']; gc.collect()\n",
    "    \n",
    "    # some other specific filter conditions (如过传入了别的前置过滤条件则执行dataframe.query)\n",
    "    if filter_ is not None:\n",
    "        data = data.query(filter_)\n",
    "        \n",
    "    print(f'end shape {data.shape}')\n",
    "    return data\n",
    "\n",
    "\n",
    "# =============================【3. Model Training Functions】=============================\n",
    "def train_single_lgb(x_train, y_train, x_valid, y_valid, fea_cols, params):\n",
    "    \"\"\"_summary_\n",
    "    Standard LGB model Traininig\n",
    "    (LGB模型训练)\n",
    "    Args:\n",
    "        x_train (datafram): training data features (训练集特征)\n",
    "        y_train (datafram): training ata labels (训练集标签)\n",
    "        x_valid (datafram): testing data features (验证集特征)\n",
    "        y_valid (datafram): testing data labels (验证集标签)\n",
    "        fea_cols (list): used columns (使用的特征)\n",
    "        params (dict): parameters of lgb (LGB模型参数)\n",
    "\n",
    "    Returns:\n",
    "        model: trained model (训练完成的LBG模型)\n",
    "        dataframe: feature importance (特征重要性df)\n",
    "    \"\"\"\n",
    "    print(\"{0:=^70}\".format(f'TRAIN LGB MODEL WITH {len(fea_cols)} FEARURES'))\n",
    "    lgb_train = lgb.Dataset(x_train[fea_cols], label = y_train)\n",
    "    lgb_valid = lgb.Dataset(x_valid[fea_cols], label = y_valid)\n",
    "    \n",
    "    lgb_clf = lgb.train(\n",
    "                params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,  # maximum iteration times (最大迭代次数)\n",
    "                valid_sets=lgb_valid,  # validation set (验证集)\n",
    "                early_stopping_rounds=50,  # early stopping (如果多少次验证集metric不增加则停止训练，并返回最高分数的模型)\n",
    "                verbose_eval = 100)\n",
    "\n",
    "    # feature importance (特征重要性)\n",
    "    imp_df = pd.DataFrame(zip(lgb_clf.feature_name(), lgb_clf.feature_importance()))\n",
    "    \n",
    "    return lgb_clf, imp_df\n",
    "\n",
    "\n",
    "# =============================【4 Model Evaluation Functions】=============================\n",
    "def model_result(clf, df_valid,top_list=[100,200,500,1000,1500,2000]):\n",
    "    \"\"\"_summary_\n",
    "    self_defined topk evaluation function.\n",
    "    (自定义TOPK效果评价函数,_d 结果按账户+天计算得到，_u 结果按账户计算得到)\n",
    "    Args:\n",
    "        clf (model): trained model (训练得到的模型)\n",
    "        df_valid (dataframe): data set to validate, test data in general (需验证效果的数据集)\n",
    "        top_list (list, optional): topk threshold, number of top k accounts with highest predict values. (TOPK阈值设定). \n",
    "                                   Defaults to [100,200,500,1000,1500,2000].\n",
    "\n",
    "    Returns:\n",
    "        dataframe: model performance wrt different thresholds (包含top_list中不同阈值下模型效果)\n",
    "    \"\"\"\n",
    "    y_proba = clf.predict(df_valid[USE_COLS])  # predict values (预测值)\n",
    "    y_proba_sort = sorted(y_proba, reverse=True)  # sorted predict values (预测值排序)\n",
    "    thresholds = [y_proba_sort[i] for i in top_list]  # predict values for different topks(各TOPK下的概率阈值)\n",
    "    \n",
    "    # return values (返回值)\n",
    "    re_dict = defaultdict(list)\n",
    "    re_dict['topk'] = top_list\n",
    "    re_dict['thresholds'] = thresholds\n",
    "    \n",
    "    # calculate evaluation scores wrt topk values (遍历阈值)\n",
    "    for thre in thresholds:\n",
    "        y_predict = [1 if i > thre else 0 for i in y_proba]  # mapping for labels under certian threshold (根据阈值对标签进行映射)\n",
    "        re_dict['precision_d'].append(precision_score(df_valid[TARGET],y_predict))  # precision (精确率)\n",
    "        re_dict['recall_d'].append(recall_score(df_valid[TARGET],y_predict))  # recall (召回率)\n",
    "        re_dict['f_score_d'].append(f1_score(df_valid[TARGET],y_predict))  # F1-score (F1值)\n",
    "        \n",
    "        ## evaluations for accounts (聚集到账户级别)\n",
    "        df_valid['predict'] = y_predict\n",
    "        df_valid_dup = df_valid.groupby(by=[UID, TARGET]).predict.max().reset_index()\n",
    "        df_valid_dup[TARGET] = df_valid_dup[TARGET].astype(int)\n",
    "        \n",
    "        re_dict['precision_u'].append(precision_score(df_valid_dup[TARGET], df_valid_dup['predict']))  # 精确率 \n",
    "        re_dict['recall_u'].append(recall_score(df_valid_dup[TARGET], df_valid_dup['predict']))  # 召回率\n",
    "        re_dict['f_score_u'].append(f1_score(df_valid_dup[TARGET], df_valid_dup['predict']))  # F1值\n",
    "        \n",
    "    re_df = pd.DataFrame(re_dict)\n",
    "    return re_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db630fda",
   "metadata": {},
   "source": [
    "# Macro Variables (宏变量配置)\n",
    "## Fixed Parameters (固定配置参数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d006b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/all_features.csv'  # feature file path (数据文件地址)\n",
    "UID = 'acct_no'  # user_id column (用户ID)\n",
    "DATE = 'oper_date'  # datetime column (日期列)\n",
    "TARGET = 'black_flag'  # label column (标签列)\n",
    "RULE_NUM = 3  # number of rules to filter (规则个数)\n",
    "\n",
    "MUST_COLS = [UID, DATE, TARGET]  # necessary columns (必要的列 一般是UID+DATE)\n",
    "# categorical features, need encoding (类别特征 一般为需要编码的特征)\n",
    "CAT_COLS = ['open_inst_no', 'id_card_number', 'if_sfz', 'gender', 'acct_opene_at']\n",
    "# columns to be romoved (需要删除的特征)\n",
    "DROP_COLS = MUST_COLS + ['id_card_number']\n",
    "# kept columns after filtering (筛选后的特征)\n",
    "USE_COLS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05648944",
   "metadata": {},
   "source": [
    "## Hyperparameter (可调节超参数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b981c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifiable hyperparameters (可调节超参数)\n",
    "RANDOM_SEED = 2022  # random seeds (随机种子)\n",
    "\n",
    "# choose number of features during training, as many as possible if mem ory allowed\n",
    "# (特征选择时每次选取多少个特征进行训练，内存不报错的情况下可以尽可能大一点)\n",
    "ONE_STEP_COLS = 500  \n",
    "\n",
    "NUM_FEATURE = 200  # number of features chosen for the final model (最终选取多少个特征)\n",
    "NULL_RATIO = 0.999  # missing value ratio threshold (缺失值大于多少时去掉该特征)\n",
    "\n",
    "# pre-filtering conditions\n",
    "# 前置过滤条件 (可以设置为None，则不进行前置过滤)\n",
    "FILTER = 'amt_sum_day > 100 and amt_cnt_day > 2'  \n",
    "\n",
    "# LGB model parameters\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',  # type of gradient boost tree (提升树类型)\n",
    "    'objective': 'binary',  # objective function (二分类目标函数)\n",
    "    'metric': 'auc',  # evaluation martix (评估指标)\n",
    "    'num_leaves': 256,  # the maximum number of leaves per tree, up to (0, 2^max_depth - 1] (每棵树的叶子节点数目)\n",
    "    'max_depth': -1,  # max depth of the tree (每棵树的最大深度(-1表示无限制))\n",
    "    'min_data_in_leaf': 50,  # the minimum number of data/sample/count per leaf (每个叶子节点所需的最小样本数)\n",
    "    'learning_rate': 0.05,  # learning_rate, deciding the update speed of weights (学习率(每次提升迭代的步长))\n",
    "    'feature_fraction': 0.8,  # feature sampling, helps avoid overfitting and speed up (每次迭代中使用的特征比例)\n",
    "    'bagging_fraction': 0.8,  # subsample, (每棵树使用的数据比例(无放回采样))\n",
    "    'bagging_freq': 5,  # bagging frequency, set with feature_fraction (采样频率(每隔多少次迭代进行一次采样))\n",
    "    'lambda_l1': 1,  # L1 regularization (正则化项)\n",
    "    'lambda_l2': 0.001,  # L2 regularization (正则化项(值越小表示正则化程度越高))\n",
    "    'min_gain_to_split': 0.2,  # minimun gain to split, helps to avoid overfitting (分裂节点所需的最小增益)\n",
    "    #'device': 'gpu',  # GPU speed up, undo the annotation when used (加速(如果使用 GPU，请取消注释))\n",
    "    'verbose': -1,  # detailed level (详细程度模式(-1 表示训练过程中不输出信息))\n",
    "    'is_unbalance': True,  # 处理类别不平衡问题，调整权重\n",
    "    'seed': RANDOM_SEED  # random seed (随机种子，用于可重复性)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36988267",
   "metadata": {},
   "source": [
    "# Preparation Works (准备工作)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9122b710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed random seed (固定随机种子)\n",
    "seed_everything(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064e551",
   "metadata": {},
   "source": [
    "## Obtain Columns (获取所有列名)\n",
    "- remove pre-set columns（去除预先设定需去除的列）\n",
    "- split and save cloumns that will be put into the model (分裂并保存规则列名)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3192c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "3641\n"
     ]
    }
   ],
   "source": [
    "# obtain columns (获取所有列)\n",
    "all_features = read_chunk_data(data_path=DATA_PATH, nrows=1, sep='\\t').columns\n",
    "# filter columns (过滤掉不需要的列)\n",
    "all_features = [fea for fea in all_features if fea not in DROP_COLS]\n",
    "print(len(all_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8deecb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns with set RULE_NUM (过滤掉规则匹配列)\n",
    "rule_cols = all_features[-RULE_NUM:]\n",
    "all_features = all_features[:-RULE_NUM]\n",
    "random.shuffle(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552645f4",
   "metadata": {},
   "source": [
    "## Data Set Split (数据集划分)\n",
    "Preprocessing data in case memory not big enough, which includes:\n",
    "- train, test split\n",
    "- pre-filtering\n",
    "- train, validation split\n",
    "\n",
    "由于数据不能一次性读入内存，因此我们需要预先对数据进行处理主要包括\n",
    "- 训练集、测试机划分\n",
    "- 前置过滤\n",
    "- 训练集、验证集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21bf755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n"
     ]
    }
   ],
   "source": [
    "# read data and filtered columns (读取全部数据的UID、DATE, 前置过滤需要用到的列)\n",
    "data = read_chunk_data(data_path=DATA_PATH, use_cols=[UID, DATE, 'amt_sum_day', 'amt_cnt_day'], chunksize=50000, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "579d15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[DATE] = pd.to_datetime(data[DATE])\n",
    "data = data[data[DATE] < pd.to_datetime('2020-08-01')]  # training data, often divided by datetime (训练集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a077d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================FILTER date==============================\n",
      "start shape (136447, 4)\n",
      "end shape (49217, 4)\n"
     ]
    }
   ],
   "source": [
    "# pre-filtering, remove accounts of low risks (前置过滤 训练集中过滤掉低风险的账户+天)\n",
    "data = filter_data(data, filter_ = FILTER)\n",
    "\n",
    "# window size can be added to acquire current trade samples \n",
    "# (可以加入时间窗口获取用户最近一段时间的样本)\n",
    "# data = filter_data(data, date_col=DATE, window=30, filter_ = FILTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5437eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train, validation sets (划分训练集、验证集)\n",
    "u_train, u_valid = train_test_split(data[[UID]].drop_duplicates(), test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "# extract filtered accounts wrt datetime (提取过滤后的账户+天)\n",
    "u_train = data[data[UID].isin(u_train[UID])][[UID, DATE]]\n",
    "u_valid = data[data[UID].isin(u_valid[UID])][[UID, DATE]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359724d4",
   "metadata": {},
   "source": [
    "# Feature Selection (特征筛选)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7b1226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdf8c5bfcab486d85667c349ce1efcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 500 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3bd49a0db0a4b4d9b7dc3585f81a83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 301 features\n",
      "====================LABEL ENCODING FOR 0 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8937db63b0054d13b10b3524bc0b2bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 199 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.955866\n",
      "[200]\tvalid_0's auc: 0.958753\n",
      "[300]\tvalid_0's auc: 0.960284\n",
      "Early stopping, best iteration is:\n",
      "[318]\tvalid_0's auc: 0.960573\n",
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 500 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edea2bff376a41aabe1983df278942fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 324 features\n",
      "====================LABEL ENCODING FOR 0 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1479fc9669f4a3a8edd0c1538178a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 336 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.957392\n",
      "[200]\tvalid_0's auc: 0.961125\n",
      "[300]\tvalid_0's auc: 0.962134\n",
      "Early stopping, best iteration is:\n",
      "[321]\tvalid_0's auc: 0.962345\n",
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 500 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc160f8ef9844aeb54d2fc08dd9542d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 310 features\n",
      "====================LABEL ENCODING FOR 1 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69c78fdf5c24aa19cf456c36186f817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 390 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.963146\n",
      "[200]\tvalid_0's auc: 0.968773\n",
      "[300]\tvalid_0's auc: 0.96997\n",
      "Early stopping, best iteration is:\n",
      "[260]\tvalid_0's auc: 0.970007\n",
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 500 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e6631f0dc946daac505d3c964ce96b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 321 features\n",
      "====================LABEL ENCODING FOR 3 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2216756fb10346359362628a4a245104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 379 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.961366\n",
      "[200]\tvalid_0's auc: 0.969159\n",
      "[300]\tvalid_0's auc: 0.970459\n",
      "Early stopping, best iteration is:\n",
      "[279]\tvalid_0's auc: 0.970493\n",
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 500 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee66150a18844a02a23264b89dde7a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 314 features\n",
      "====================LABEL ENCODING FOR 3 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1afc8b822b59409b8dbff92d0f83c266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 386 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.9614\n",
      "[200]\tvalid_0's auc: 0.968093\n",
      "[300]\tvalid_0's auc: 0.969376\n",
      "Early stopping, best iteration is:\n",
      "[285]\tvalid_0's auc: 0.969432\n",
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 500 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccbd9692e6b6407d945f0b649410d2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 309 features\n",
      "====================LABEL ENCODING FOR 3 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66adf13112724594ab3b0c2b1561227e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 391 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.96186\n",
      "[200]\tvalid_0's auc: 0.967956\n",
      "[300]\tvalid_0's auc: 0.969345\n",
      "Early stopping, best iteration is:\n",
      "[310]\tvalid_0's auc: 0.969434\n",
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 500 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b469fcdef0444a9e2eeb82696ef5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 318 features\n",
      "====================LABEL ENCODING FOR 3 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d14f763b8c8240efa0a006539694aa27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 382 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.963526\n",
      "[200]\tvalid_0's auc: 0.969178\n",
      "[300]\tvalid_0's auc: 0.970771\n",
      "Early stopping, best iteration is:\n",
      "[283]\tvalid_0's auc: 0.97084\n",
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=========================FILTER 137 FEARURES==========================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3122ad711224c668f2851cd25795859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drop 90 features\n",
      "====================LABEL ENCODING FOR 3 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed79e632e484fcaa23b462ff133dbe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 247 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.965282\n",
      "[200]\tvalid_0's auc: 0.97053\n",
      "[300]\tvalid_0's auc: 0.971624\n",
      "Early stopping, best iteration is:\n",
      "[281]\tvalid_0's auc: 0.971684\n"
     ]
    }
   ],
   "source": [
    "best_auc = 0  # store the best auc score (最佳AUC指标初始值)\n",
    "\n",
    "# in case of memory shortage, train the data using fixed number of features\n",
    "# (按照固定步长循环遍历特征列表)\n",
    "for i in tqdm_notebook(range(len(all_features) // ONE_STEP_COLS + 1)):\n",
    "    tmp_cols = all_features[i*ONE_STEP_COLS: min(len(all_features) - 1, (i + 1) * ONE_STEP_COLS)]\n",
    "    USE_COLS += tmp_cols  # Add feature columns to USE_COLS list (将当前步长的特征添加到使用的特征列表中)\n",
    "    \n",
    "    # Read data chunks, preprocessing first(fill in missing values)\n",
    "    # (读取分块数据，并进行数据预处理)\n",
    "    data = read_chunk_data(data_path=DATA_PATH, use_cols=USE_COLS+MUST_COLS, chunksize=50000, sep='\\t')\n",
    "    data[TARGET] = data[TARGET].fillna(0).astype(int)\n",
    "    data[DATE] = pd.to_datetime(data[DATE])\n",
    "    data = data[data[DATE] < pd.to_datetime('2020-08-01')]  # training data (过滤出训练集数据)\n",
    "    \n",
    "    # filter feature columns wrt filtering conditions (根据条件过滤特征列)\n",
    "    data, USE_COLS = filter_cols(data, USE_COLS, tmp_cols, NULL_RATIO)\n",
    "    \n",
    "    # encoding categorical features (对分类特征进行标签编码)\n",
    "    data, map_dict = label_encode(data, [i for i in USE_COLS if i in CAT_COLS])\n",
    "    \n",
    "    # concat train set (拼接训练集)\n",
    "    x_train = u_train.merge(data, how='left', on=[UID, DATE])\n",
    "    y_train = x_train[TARGET].fillna(0).astype('int32')\n",
    "\n",
    "    #concat val set (拼接验证集)\n",
    "    x_valid = u_valid.merge(data, how='left', on=[UID, DATE])\n",
    "    y_valid = x_valid[TARGET].fillna(0).astype('int32')\n",
    "    \n",
    "    # Using Lightgbm to train + predict (使用LightGBM进行训练和预测)\n",
    "    lgb_clf, imp = train_single_lgb(x_train, y_train, x_valid, y_valid, USE_COLS, params)\n",
    "    \n",
    "    cur_auc = lgb_clf.best_score['valid_0']['auc']  # AUC for current iteration (当前轮次的AUC指标)\n",
    "    if cur_auc > best_auc:  # choose the best auc (如果当前AUC指标优于最佳AUC指标)\n",
    "        # choose the most important features (选择重要性排名靠前的特征作为最终使用的特征)\n",
    "        USE_COLS = imp[imp[1] > 10].sort_values(by=1, ascending=False).head(NUM_FEATURE)[0].to_list()\n",
    "        best_auc = cur_auc  # update better auc score (更新最佳AUC指标)\n",
    "    else:\n",
    "        # if auc no improvement, remove features of current steps\n",
    "        # (如果当前AUC指标没有提升，则移除本轮步长添加的特征)\n",
    "        USE_COLS = [col for col in USE_COLS if col not in tmp_cols]\n",
    "        \n",
    "    del data, x_train, x_valid  # memory clean up (删除临时变量，释放内存)\n",
    "    gc.collect()  # garbage colelction (执行垃圾回收)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33af9d",
   "metadata": {},
   "source": [
    "# Check Performance (效果验证)\n",
    "## Model Training (训练模型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fee7318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============LOADING DATA FROM : ../data/all_features.csv=============\n",
      "=============================FILTER date==============================\n",
      "start shape (136447, 3644)\n",
      "end shape (49217, 3644)\n",
      "过滤前测试集： (59879, 3644)\n",
      "过滤后测试集： (58138, 3644)\n"
     ]
    }
   ],
   "source": [
    "# checking performance (测试效果)\n",
    "data = read_chunk_data(data_path=DATA_PATH, use_cols=all_features+rule_cols+MUST_COLS, chunksize=50000, sep='\\t')\n",
    "data[TARGET] = data[TARGET].fillna(0).astype(int)\n",
    "\n",
    "data[DATE] = pd.to_datetime(data[DATE])\n",
    "# Training data and test data are usually divided according to time\n",
    "# Historical transactions for training, current transactions(e.g latest several months) are\n",
    "# used for test.\n",
    "test = data[data[DATE] > pd.to_datetime('2020-07-31')]  # test data set (测试集)\n",
    "data = data[data[DATE] < pd.to_datetime('2020-08-01')]  # training data set (训练集)\n",
    "\n",
    "# filter features same as previous operations (根据条件过滤特征列)\n",
    "data = filter_data(data, filter_ = FILTER)\n",
    "\n",
    "# exclude black samples (labeled) 训练集的黑样本不能出现在测试集中\n",
    "print('过滤前测试集：', test.shape)\n",
    "test = test[~test[UID].isin(data[data[TARGET] == 1][UID].unique())]\n",
    "print('过滤后测试集：', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02ac312f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================LABEL ENCODING FOR 3 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a11bdc89bf3d4ab4a79ebf34ec4e184d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================LABEL ENCODING FOR 3 FEARURES=====================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3557506bc32d408b9d8e514ad8eed53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# encoding categorical features (类别编码)\n",
    "data, map_dict = label_encode(data, [i for i in USE_COLS if i in CAT_COLS])\n",
    "test, _ = label_encode(test, [i for i in USE_COLS if i in CAT_COLS], map_dict=map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "620b2d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat train and val set respectively(拼接训练集和验证集)\n",
    "x_train = u_train.merge(data, how='left', on=[UID, DATE])\n",
    "y_train = x_train[TARGET].fillna(0).astype('int32')\n",
    "\n",
    "x_valid = u_valid.merge(data, how='left', on=[UID, DATE])\n",
    "y_valid = x_valid[TARGET].fillna(0).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84416fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================TRAIN LGB MODEL WITH 200 FEARURES===================\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\tvalid_0's auc: 0.962729\n",
      "[200]\tvalid_0's auc: 0.969476\n",
      "[300]\tvalid_0's auc: 0.970919\n",
      "Early stopping, best iteration is:\n",
      "[311]\tvalid_0's auc: 0.971062\n"
     ]
    }
   ],
   "source": [
    "lgb_clf, imp = train_single_lgb(x_train, y_train, x_valid, y_valid, USE_COLS, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66d2833a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n_oppo_acct_bank_id_in_7d</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work_time_trans_sum_ratio_1m</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>close_100int_sum_ratio_15d</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>khsc_year</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>end_balance_3m</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>work_time_trans_cnt_ratio_15d</td>\n",
       "      <td>321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>close_int_sum_ratio_15d</td>\n",
       "      <td>303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_oppo_acct_bank_id_7d</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>amt_less10_cnt_ratio_3m</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>out_amt_less500_sum_3m</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>amt_decimal_cnt_ratio_3m</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>acct_opene_at</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>open_inst_no</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n_oper_amount_out_day</td>\n",
       "      <td>219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>balance_less200_sum_ratio_1m</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>close_100int_cnt_ratio_15d</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>night_23_1_trans_cnt_ratio_2m</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>in_amt_less500_sum_7d</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>out_work_time_trans_cnt_7d</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>start_balance_2m</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>balance_less200_sum_15d</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>amt_more2000_sum_ratio_3m</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>close_100int_sum_ratio_1m</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spe_amt_sum_ratio_3m</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>in_balance_less20_sum_2m</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>balance_less50_cnt_ratio_15d</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>n_oper_amount_2m</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>n_oppo_acct_bank_id_out_1m</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>balance_less10_sum_1m</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>amt_cnt_day</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>balance_less10_cnt_ratio_15d</td>\n",
       "      <td>134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>work_time_trans_sum_1m</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>work_time_trans_sum_ratio_2m</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>amt_less20_cnt_ratio_3m</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>in_amt_less10_sum_3m</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>out_amt_less100_sum_1m</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>out_amt_less200_sum_15d</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>amt_less500_sum_3m</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>in_amt_more2000_cnt_2m</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>out_close_100int_sum_15d</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>in_balance_less10_sum_1m</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>amt_more50000_sum_ratio_3m</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>out_balance_less100_sum_1m</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>out_amt_cnt_7d</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>balance_less100_cnt_ratio_2m</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>amt_more1000_sum_ratio_15d</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>amt_more5000_sum_ratio_3m</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>out_work_time_trans_sum_3m</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>amt_more1000_cnt_ratio_1m</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>n_oppo_acct_bank_id_in_3m</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0    1\n",
       "0       n_oppo_acct_bank_id_in_7d  610\n",
       "1    work_time_trans_sum_ratio_1m  555\n",
       "2      close_100int_sum_ratio_15d  495\n",
       "3                       khsc_year  450\n",
       "4                  end_balance_3m  327\n",
       "6   work_time_trans_cnt_ratio_15d  321\n",
       "8         close_int_sum_ratio_15d  303\n",
       "7          n_oppo_acct_bank_id_7d  297\n",
       "9         amt_less10_cnt_ratio_3m  296\n",
       "5          out_amt_less500_sum_3m  290\n",
       "10       amt_decimal_cnt_ratio_3m  276\n",
       "11                  acct_opene_at  272\n",
       "17                   open_inst_no  221\n",
       "13          n_oper_amount_out_day  219\n",
       "14   balance_less200_sum_ratio_1m  218\n",
       "12     close_100int_cnt_ratio_15d  204\n",
       "18  night_23_1_trans_cnt_ratio_2m  194\n",
       "15          in_amt_less500_sum_7d  185\n",
       "16     out_work_time_trans_cnt_7d  170\n",
       "21               start_balance_2m  166\n",
       "19        balance_less200_sum_15d  163\n",
       "20      amt_more2000_sum_ratio_3m  158\n",
       "33      close_100int_sum_ratio_1m  156\n",
       "27           spe_amt_sum_ratio_3m  151\n",
       "39       in_balance_less20_sum_2m  144\n",
       "24   balance_less50_cnt_ratio_15d  143\n",
       "42               n_oper_amount_2m  142\n",
       "29     n_oppo_acct_bank_id_out_1m  141\n",
       "37          balance_less10_sum_1m  140\n",
       "23                    amt_cnt_day  139\n",
       "22   balance_less10_cnt_ratio_15d  134\n",
       "26         work_time_trans_sum_1m  130\n",
       "54   work_time_trans_sum_ratio_2m  128\n",
       "43        amt_less20_cnt_ratio_3m  127\n",
       "45           in_amt_less10_sum_3m  127\n",
       "28         out_amt_less100_sum_1m  125\n",
       "36        out_amt_less200_sum_15d  122\n",
       "35             amt_less500_sum_3m  121\n",
       "41         in_amt_more2000_cnt_2m  119\n",
       "47       out_close_100int_sum_15d  119\n",
       "31       in_balance_less10_sum_1m  116\n",
       "40     amt_more50000_sum_ratio_3m  112\n",
       "32     out_balance_less100_sum_1m  109\n",
       "50                 out_amt_cnt_7d  109\n",
       "69   balance_less100_cnt_ratio_2m  104\n",
       "25     amt_more1000_sum_ratio_15d  104\n",
       "30      amt_more5000_sum_ratio_3m  103\n",
       "61     out_work_time_trans_sum_3m  103\n",
       "34      amt_more1000_cnt_ratio_1m  102\n",
       "66      n_oppo_acct_bank_id_in_3m  100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imp.sort_values(by=1, ascending=False).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd7e8e",
   "metadata": {},
   "source": [
    "## TOPK Result (TOPK效果)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c009f74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topk</th>\n",
       "      <th>thresholds</th>\n",
       "      <th>precision_d</th>\n",
       "      <th>recall_d</th>\n",
       "      <th>f_score_d</th>\n",
       "      <th>precision_u</th>\n",
       "      <th>recall_u</th>\n",
       "      <th>f_score_u</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.265252</td>\n",
       "      <td>0.419287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.411290</td>\n",
       "      <td>0.582857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200</td>\n",
       "      <td>0.998649</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.530504</td>\n",
       "      <td>0.693241</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.701613</td>\n",
       "      <td>0.824645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>0.862581</td>\n",
       "      <td>0.702000</td>\n",
       "      <td>0.931034</td>\n",
       "      <td>0.800456</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.967742</td>\n",
       "      <td>0.905660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>0.361564</td>\n",
       "      <td>0.363000</td>\n",
       "      <td>0.962865</td>\n",
       "      <td>0.527233</td>\n",
       "      <td>0.614213</td>\n",
       "      <td>0.975806</td>\n",
       "      <td>0.753894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1500</td>\n",
       "      <td>0.106760</td>\n",
       "      <td>0.244163</td>\n",
       "      <td>0.970822</td>\n",
       "      <td>0.390192</td>\n",
       "      <td>0.463878</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.630491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.183000</td>\n",
       "      <td>0.970822</td>\n",
       "      <td>0.307951</td>\n",
       "      <td>0.347578</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.513684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topk  thresholds  precision_d  recall_d  f_score_d  precision_u  recall_u  \\\n",
       "0   100    0.999625     1.000000  0.265252   0.419287     1.000000  0.411290   \n",
       "1   200    0.998649     1.000000  0.530504   0.693241     1.000000  0.701613   \n",
       "2   500    0.862581     0.702000  0.931034   0.800456     0.851064  0.967742   \n",
       "3  1000    0.361564     0.363000  0.962865   0.527233     0.614213  0.975806   \n",
       "4  1500    0.106760     0.244163  0.970822   0.390192     0.463878  0.983871   \n",
       "5  2000    0.049765     0.183000  0.970822   0.307951     0.347578  0.983871   \n",
       "\n",
       "   f_score_u  \n",
       "0   0.582857  \n",
       "1   0.824645  \n",
       "2   0.905660  \n",
       "3   0.753894  \n",
       "4   0.630491  \n",
       "5   0.513684  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_result(lgb_clf,test,top_list=[100,200,500,1000,1500,2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2e3deef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9959541347654124"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = lgb_clf.predict(test[USE_COLS])\n",
    "test['preds'] = preds\n",
    "pre_max = test.groupby(UID)['preds'].idxmax()\n",
    "preds_df = test.loc[pre_max, [UID, 'preds', TARGET, DATE]]\n",
    "\n",
    "roc_auc_score(preds_df[TARGET], preds_df['preds']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c1be7",
   "metadata": {},
   "source": [
    "## Rule Explaination (规则解释)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d06d554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acct_no</th>\n",
       "      <th>preds</th>\n",
       "      <th>black_flag</th>\n",
       "      <th>oper_date</th>\n",
       "      <th>yjpf</th>\n",
       "      <th>fszc</th>\n",
       "      <th>jzzc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>9EEF218E52B7B373</td>\n",
       "      <td>0.999949</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>477C783DB38D79FC</td>\n",
       "      <td>0.999926</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-08-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>4B23C476C5003ECF</td>\n",
       "      <td>0.999916</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1763</th>\n",
       "      <td>63ECD0B0202D9F7F</td>\n",
       "      <td>0.999913</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>D21C2B6F3BDD4392</td>\n",
       "      <td>0.999908</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-08-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>C1FDF04B9D6AF4DA</td>\n",
       "      <td>0.368743</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>集中转入，分散转出</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4107</th>\n",
       "      <td>E68032544AFDCFC6</td>\n",
       "      <td>0.366435</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2513</th>\n",
       "      <td>8DDB0B1013EE7C5F</td>\n",
       "      <td>0.358697</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>45F7995529FE0A3F</td>\n",
       "      <td>0.339936</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>6C24D922A92796C1</td>\n",
       "      <td>0.335624</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               acct_no     preds  black_flag  oper_date yjpf       fszc jzzc\n",
       "2806  9EEF218E52B7B373  0.999949           1 2020-08-21  NaN        NaN    0\n",
       "1249  477C783DB38D79FC  0.999926           1 2020-08-06  NaN        NaN    0\n",
       "1313  4B23C476C5003ECF  0.999916           1 2020-08-21  NaN        NaN    0\n",
       "1763  63ECD0B0202D9F7F  0.999913           1 2020-08-18  NaN        NaN    0\n",
       "3729  D21C2B6F3BDD4392  0.999908           1 2020-08-10  NaN        NaN    0\n",
       "...                ...       ...         ...        ...  ...        ...  ...\n",
       "3462  C1FDF04B9D6AF4DA  0.368743           0 2020-08-23  NaN  集中转入，分散转出    0\n",
       "4107  E68032544AFDCFC6  0.366435           0 2020-08-22  NaN        NaN    0\n",
       "2513  8DDB0B1013EE7C5F  0.358697           0 2020-08-10  NaN        NaN    0\n",
       "1229  45F7995529FE0A3F  0.339936           0 2020-08-16  NaN        NaN    0\n",
       "1922  6C24D922A92796C1  0.335624           0 2020-08-23  NaN        NaN    0\n",
       "\n",
       "[200 rows x 7 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df = preds_df.merge(test[[UID, DATE] + rule_cols], how='left', on=[UID, DATE])\n",
    "preds_df.sort_values(by='preds', ascending=False).head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5841ad24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "258.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
